---
apiVersion: v1
kind: Namespace
metadata:
  name: inference-gateway
  labels:
    inference-gateway.com/managed: "true"
---
apiVersion: core.inference-gateway.com/v1alpha1
kind: Gateway
metadata:
  name: inference-gateway
  namespace: inference-gateway
spec:
  # Core gateway configuration
  environment: development
  hpa:
    enabled: false
    config:
      minReplicas: 3
      maxReplicas: 10
      metrics:
        - type: Resource
          resource:
            name: cpu
            target:
              type: Utilization
              averageUtilization: 80
      # TODO - I need to let the operator create this reference, so the user doesn't need to think about it
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: inference-gateway
  image: "ghcr.io/inference-gateway/inference-gateway:0.12.0"

  # OpenTelemetry observability configuration
  telemetry:
    enabled: true
    metrics:
      enabled: true
      port: 9464

  # Server configuration
  server:
    host: "0.0.0.0"
    # port: 8080
    timeouts:
      read: "60s"
      write: "60s"
      idle: "300s"
    tls:
      enabled: false
      # Default behavior: If enabled is true and no custom secret is specified, the controller will automatically mount the
      # same TLS secret used by the Ingress (inference-gateway-tls) into the backend pod for end-to-end TLS.
      # To override, specify certificateRef and keyRef explicitly.

  # Authentication configuration
  auth:
    enabled: false
    provider: oidc
    oidc:
      issuerUrl: "https://keycloak.inference-gateway.local/realms/inference-gateway-realm"
      clientId: "inference-gateway-client"
      clientSecretRef:
        name: inference-gateway-secrets
        key: OIDC_CLIENT_SECRET

  # AI Provider configurations
  providers:
    - name: OpenAI
      env:
        - name: OPENAI_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: OPENAPI_API_URL
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: OPENAI_API_KEY
    - name: Anthropic
      env:
        - name: ANTHROPIC_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: ANTHROPIC_API_URL
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: ANTHROPIC_API_KEY
    - name: Groq
      env:
        - name: GROQ_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: GROQ_API_URL
        - name: GROQ_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: GROQ_API_KEY
    - name: Cohere
      env:
        - name: COHERE_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: COHERE_API_URL
        - name: COHERE_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: COHERE_API_KEY
    - name: Cloudflare
      env:
        - name: CLOUDFLARE_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: CLOUDFLARE_API_URL
        - name: CLOUDFLARE_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: CLOUDFLARE_API_KEY
    - name: DeepSeek
      env:
        - name: DEEPSEEK_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: DEEPSEEK_API_URL
        - name: DEEPSEEK_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: DEEPSEEK_API_KEY
    - name: Ollama
      env:
        - name: OLLAMA_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: OLLAMA_API_URL
        - name: OLLAMA_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: OLLAMA_API_KEY
    - name: Custom
      env:
        - name: CUSTOM_API_URL
          valueFrom:
            configMapKeyRef:
              name: inference-gateway-config
              key: CUSTOM_API_URL
        - name: CUSTOM_API_KEY
          valueFrom:
            secretKeyRef:
              name: inference-gateway-providers-secret
              key: CUSTOM_API_KEY

  # Model Context Protocol configuration
  mcp:
    enabled: false
    expose: false
    timeouts:
      client: "10s"
      dial: "5s"
      tlsHandshake: "5s"
      responseHeader: "5s"
      request: "10s"
    servers:
      - name: filesystem-server
        url: "http://mcp-filesystem.mcp.svc.cluster.local:8080"
        healthCheck:
          enabled: true
          path: "/health"
          interval: "30s"
      - name: git-server
        url: "http://mcp-git.mcp.svc.cluster.local:8080"
        healthCheck:
          enabled: true
          path: "/health"
          interval: "30s"

  # Agent-to-Agent configuration
  a2a:
    enabled: false
    expose: false
    timeouts:
      client: "60s"
    polling:
      enabled: true
      interval: "2s"
      timeout: "60s"
      maxAttempts: 30
    agents:
      - name: google-calendar-agent
        url: "http://google-calendar-agent.agents.svc.cluster.local:8080"
        healthCheck:
          enabled: true
          path: "/health"
          interval: "30s"
      - name: email-agent
        url: "http://email-agent.agents.svc.cluster.local:8080"
        healthCheck:
          enabled: true
          path: "/health"
          interval: "30s"

  # Resource management
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "1000m"
      memory: "512Mi"

  # Ingress configuration
  ingress:
    enabled: true
    host: "api.inference-gateway.local"
    annotations:
      cert-manager.io/cluster-issuer: "selfsigned-cluster-issuer"
    tls:
      enabled: true
      secretName: inference-gateway-tls
---
apiVersion: v1
kind: Secret
metadata:
  name: inference-gateway-providers-secret
  namespace: inference-gateway
type: Opaque
stringData:
  OIDC_CLIENT_SECRET: ""

  OPENAI_API_KEY: ""
  ANTHROPIC_API_KEY: ""
  GROQ_API_KEY: ""
  COHERE_API_KEY: ""
  CLOUDFLARE_API_KEY: ""
  DEEPSEEK_API_KEY: "test"
  OLLAMA_API_KEY: ""
