---
apiVersion: v1
kind: Namespace
metadata:
  name: inference-gateway
  labels:
    inference-gateway.com/managed: "true"
---
apiVersion: core.inference-gateway.com/v1alpha1
kind: Gateway
metadata:
  name: inference-gateway
  namespace: inference-gateway
  labels:
    app.kubernetes.io/name: inference-gateway
    app.kubernetes.io/instance: inference-gateway
    app.kubernetes.io/component: api-gateway
    app.kubernetes.io/part-of: inference-gateway
    app.kubernetes.io/version: "0.12.0"
spec:
  # Core gateway configuration
  environment: development
  image: "ghcr.io/inference-gateway/inference-gateway:0.12.0"

  # AI Provider configurations
  providers:
    - name: openai
      type: openai
      config:
        baseUrl: "https://api.openai.com/v1"
        authType: bearer
        tokenRef:
          name: inference-gateway-secrets
          key: OPENAI_API_KEY

  # Simple ingress configuration - minimal setup
  ingress:
    enabled: true
    host: "api.inference-gateway.local"
    # Defaults:
    # - className: "nginx" (auto-detected)
    # - path: "/" with pathType: "Prefix"
    # - TLS enabled with auto-generated secret name
    # - cert-manager issuer based on environment (development = selfsigned-issuer)

  # Resource management
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"
---
apiVersion: v1
kind: Secret
metadata:
  name: inference-gateway-secrets
  namespace: inference-gateway
  labels:
    app.kubernetes.io/name: inference-gateway
    app.kubernetes.io/instance: inference-gateway
    app.kubernetes.io/component: secrets
    app.kubernetes.io/part-of: inference-gateway
    app.kubernetes.io/version: "0.12.0"
type: Opaque
stringData:
  OPENAI_API_KEY: ""
